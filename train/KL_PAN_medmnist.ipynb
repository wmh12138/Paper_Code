{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_hZ2BCTioSb"
      },
      "source": [
        "设置工作目录"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IOQM0F_9iHRH",
        "outputId": "089702fc-3cc4-46dd-9b2d-844149248810"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/My Drive/H-PAN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "os.chdir(\"/KL_PAN_medmnist/\")\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzj1w8OalVwj"
      },
      "source": [
        "引入辅助函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEMlDOGwlSkg"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "sys.path.append(\"/KL_PAN_medmnist/\")\n",
        "import helper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6aMS2SPsIjx"
      },
      "outputs": [],
      "source": [
        "#from helper import cfg_helper\n",
        "#from helper import file_helper\n",
        "#from helper import math_helper\n",
        "#from helper import pu_learning_dataset\n",
        "# from helper import ploting_helper\n",
        "\n",
        "__author__ = 'garrett_local'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yswlRLERydm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import pickle\n",
        "import urllib.request as request\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "__author__ = 'garrett_local'\n",
        "\n",
        "\n",
        "def last_modified(path_to_match):\n",
        "    \"\"\"\n",
        "    Get the last modified file among those matching with a given wildcards.\n",
        "    :param path_to_match: a wildcards indicating what files to find.\n",
        "    :return: the last modified file among those matching with a given wildcards.\n",
        "    \"\"\"\n",
        "    matched = glob.glob(path_to_match)\n",
        "    if len(matched) == 0:\n",
        "        return None\n",
        "    matched.sort(key=lambda f: os.stat(f).st_mtime)\n",
        "    newest = matched[-1]\n",
        "    return newest\n",
        "\n",
        "\n",
        "def create_dirname_if_not_exist(path):\n",
        "    create_if_not_exist(os.path.dirname(path))\n",
        "\n",
        "\n",
        "def create_if_not_exist(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        c = pickle.load(fo, encoding='latin1')\n",
        "    return c\n",
        "\n",
        "\n",
        "def download(url, des_path='.'):\n",
        "    try:\n",
        "        print('downloading {}'.format(url))\n",
        "        request.urlretrieve(url, os.path.join(des_path,\n",
        "                                              os.path.basename(url)))\n",
        "        request.urlcleanup()\n",
        "    except request.HTTPError as e:\n",
        "        print('HTTP Error: {} {}'.format(e.code, url))\n",
        "    except request.URLError as e:\n",
        "        print('URL Error: {} {}'.format(e.reason, url))\n",
        "\n",
        "\n",
        "def get_unique_name():\n",
        "    return '{}'.format(int(time.time()))\n",
        "\n",
        "\n",
        "def read_exp_log_data(exp_name):\n",
        "    path = 'result/{}/log/summaries.pkl'.format(exp_name)\n",
        "    if os.path.exists(path):\n",
        "        with open(path, 'rb') as f:\n",
        "            log_data = pickle.load(f, encoding='latin1')\n",
        "    else:\n",
        "        path = 'result/{}/log/summaries.npz'.format(exp_name)\n",
        "        d = np.load(path)\n",
        "        log_data = LogData()\n",
        "        for key in d.keys():\n",
        "            setattr(log_data, key, d[key])\n",
        "    return log_data\n",
        "\n",
        "\n",
        "def save_basetrings_as_text(basestrings, file_name):\n",
        "    create_dirname_if_not_exist(file_name)\n",
        "    with open(file_name, 'wb') as f:\n",
        "        for s in basestrings:\n",
        "            f.write(s.encode()+b'\\n')\n",
        "\n",
        "\n",
        "def load_basestrings(file_name):\n",
        "    with open(file_name, 'r') as f:\n",
        "        basestrings = f.readlines()\n",
        "    return [s.strip('\\n') for s in basestrings]\n",
        "\n",
        "\n",
        "def save_log_data(log_data, exp_name):\n",
        "    path = './result/tmp/{}/log'.format(exp_name)\n",
        "    create_if_not_exist(path)\n",
        "    with open(os.path.join(path, 'summaries.pkl'), 'wb') as f:\n",
        "        pickle.dump(log_data, f, protocol=2)\n",
        "\n",
        "\n",
        "def settle_saved_data(exp_name):\n",
        "    shutil.move('./result/tmp/{}'.format(exp_name),\n",
        "                './result/{}'.format(exp_name),)\n",
        "\n",
        "\n",
        "class LogData(object):\n",
        "    def __init__(self):\n",
        "        self.losses = {}\n",
        "\n",
        "    def __str__(self):\n",
        "        return '{}'.format(self.__dict__.keys())\n",
        "\n",
        "    def log_loss(self, key, loss):\n",
        "        if key not in self.losses:\n",
        "            self.losses[key] = [loss]\n",
        "        else:\n",
        "            self.losses[key].append(loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CclQgkSdSkaI"
      },
      "outputs": [],
      "source": [
        "import configparser\n",
        "\n",
        "\n",
        "__author__ = 'garrett_local'\n",
        "\n",
        "\n",
        "def write_cfg_file(path, cfg):\n",
        "    \"\"\"\n",
        "    Write configuration to a txt file.\n",
        "    :param path: basestring. Indicating where to save the file.\n",
        "    :param cfg: configuration. Must be a dict containing a few of dict. Each\n",
        "                dict contained is content of a section. They contain a few\n",
        "                items, whose value should be basestring.\n",
        "    \"\"\"\n",
        "    conf = configparser.ConfigParser()\n",
        "    create_dirname_if_not_exist(path)\n",
        "    cfg_file = open(path, 'w')\n",
        "    sections = cfg.keys()\n",
        "    sections = sorted(sections)\n",
        "    for section in sections:\n",
        "        content = cfg[section]\n",
        "        conf.add_section(section)\n",
        "        items_names = content.keys()\n",
        "        items_names = sorted(items_names)\n",
        "        for item_name in items_names:\n",
        "            conf.set(section, item_name, str(content[item_name]))\n",
        "    conf.write(cfg_file)\n",
        "    cfg_file.close()\n",
        "\n",
        "\n",
        "def write_exp_cfg_file(cfg, exp_name, cfg_name):\n",
        "    path = './result/tmp/{}/cfg/{}'.format(exp_name, cfg_name)\n",
        "    write_cfg_file(path, cfg)\n",
        "\n",
        "\n",
        "def read_cfg_file(path):\n",
        "    \"\"\"\n",
        "    Read configuration saved by function write_cfg_file.\n",
        "    :param path: basestring. Where to find the configuration file.\n",
        "    :return: configuration. It is a dict containing a few of dict. Each dict\n",
        "                contained is content of a section. They contain a few items,\n",
        "                whose value will be basestring.\n",
        "    \"\"\"\n",
        "    conf = configparser.ConfigParser()\n",
        "    conf.read(path)\n",
        "    sections = conf.sections()\n",
        "    cfg = {}\n",
        "    if len(sections) == 0:\n",
        "        raise AssertionError('sections is empty. File {} may not exist or may '\n",
        "                             'be empty'.format(path))\n",
        "    for section in sections:\n",
        "        options = conf.options(section)\n",
        "        cfg_section = {}\n",
        "        for option in options:\n",
        "            item = conf.get(section, option)\n",
        "            cfg_section[option] = item\n",
        "        cfg[section] = cfg_section\n",
        "    return cfg\n",
        "\n",
        "\n",
        "def to_bool(s):\n",
        "    if isinstance(s, bool):\n",
        "        return s\n",
        "    elif isinstance(s, str):\n",
        "        return True if s == 'True' else False\n",
        "    else:\n",
        "        raise ValueError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_trMOdRTU9yM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "__author__ = 'garrett_local'\n",
        "\n",
        "\n",
        "def mask_to_index(mask):\n",
        "    index = np.array(range(len(mask)))[mask]\n",
        "    return index\n",
        "\n",
        "\n",
        "def index_to_mask(index, mask_len):\n",
        "    mask = np.zeros(mask_len).astype(np.bool)\n",
        "    mask[index] = True\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _crop_common_part(arr1, arr2):\n",
        "    s1 = arr1.shape\n",
        "    s2 = arr2.shape\n",
        "    assert len(s1) == len(s2)\n",
        "    slc = [slice(0, min(s1[i], s2[i])) for i in range(len(s1))]\n",
        "    new_mask1 = arr1[slc]\n",
        "    new_mask2 = arr2[slc]\n",
        "    return new_mask1, new_mask2\n",
        "\n",
        "\n",
        "def logical_and(mask1, mask2):\n",
        "    return np.logical_and(*_crop_common_part(mask1, mask2))\n",
        "\n",
        "\n",
        "def logical_or(mask1, mask2):\n",
        "    return np.logical_or(*_crop_common_part(mask1, mask2))\n",
        "\n",
        "\n",
        "def normalize(arr):\n",
        "    return arr / np.sum(arr).astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7994yEp77VI7"
      },
      "outputs": [],
      "source": [
        "import abc\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "__author__ = 'garrett_local'\n",
        "\n",
        "\n",
        "class DataIterator(object):\n",
        "    def __init__(self, data_lists, batch_size, max_epoch=None, repeat=True,\n",
        "                 shuffle=True, epoch_finished=None):\n",
        "        for idx in range(len(data_lists) - 1):\n",
        "            assert len(data_lists[idx]) == len(data_lists[idx + 1])\n",
        "        self._data = data_lists\n",
        "        self._batch_size = batch_size\n",
        "        self._repeat = repeat\n",
        "        self._shuffle = shuffle\n",
        "        self._num_data = len(self._data[0])\n",
        "        assert self._num_data >= self._batch_size\n",
        "        self._shuffle_indexes = self._maybe_generate_shuffled_indexes()\n",
        "        # 这是用来控制是否重复一套训练，控制阀是检验epoch是否超过设置的最大迭代次数\n",
        "        self._epoch_finished = 0 if epoch_finished is None else epoch_finished\n",
        "        self._max_epoch = max_epoch\n",
        "\n",
        "    @property\n",
        "    def num_data(self):\n",
        "        return self._num_data\n",
        "\n",
        "    @property\n",
        "    def finished(self):\n",
        "        if not self._repeat:\n",
        "            if self.epoch_finished == 1:\n",
        "                return True\n",
        "        if self._max_epoch is not None:\n",
        "            return self.epoch_finished > self._max_epoch\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    @property\n",
        "    def epoch_finished(self):\n",
        "        return self._epoch_finished\n",
        "    #生成乱序标签\n",
        "    def _maybe_generate_shuffled_indexes(self):\n",
        "        indexes = list(range(self._num_data))\n",
        "        if self._shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "        return indexes\n",
        "\n",
        "    def get_next_batch(self, batch_size=None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self._batch_size\n",
        "        else:\n",
        "            assert self._num_data >= batch_size\n",
        "        #定义一个异常，用raise 抛出StopIteration()\n",
        "        if len(self._shuffle_indexes) == 0:\n",
        "            raise StopIteration()\n",
        "        if len(self._shuffle_indexes) >= batch_size:  # when data left is enough\n",
        "            indexes = self._shuffle_indexes[:batch_size]\n",
        "            #这一步是为了下一次迭代做准备，把已经用过的数据抛弃掉\n",
        "            self._shuffle_indexes = self._shuffle_indexes[batch_size:]\n",
        "        else:  # when data left is not enough.\n",
        "            indexes = self._shuffle_indexes\n",
        "            self._shuffle_indexes = []\n",
        "        # 一次训练是否完成的条件是：剩下的数据量小于给定的批次量。\n",
        "        #当一次训练完成后，需要通过随机打乱函数重新生成随机的索引\n",
        "        if len(self._shuffle_indexes) == 0:\n",
        "            self._epoch_finished += 1\n",
        "            if self._repeat:\n",
        "                if self._max_epoch is not None:\n",
        "                    if self._epoch_finished > self._max_epoch:\n",
        "                        raise StopIteration()\n",
        "                self._shuffle_indexes = self._maybe_generate_shuffled_indexes()\n",
        "                # 当数据量不够一个批次的训练时，需要从self._shuffle_indexes的前面开始截取缺少的部分\n",
        "                num_left = batch_size - len(indexes)\n",
        "                #extend () 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）\n",
        "                indexes.extend(self._shuffle_indexes[:num_left])\n",
        "                #截取过的数据不再使用\n",
        "                self._shuffle_indexes = self._shuffle_indexes[num_left:]\n",
        "        return [l[indexes] for l in self._data]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        return self.get_next_batch()\n",
        "\n",
        "\n",
        "class PuDataIterator(object):\n",
        "    def __init__(self, u_data, l_data, batch_size, max_epoch=None,\n",
        "                 epoch_finished=0, repeat=True, shuffle=True):\n",
        "        #the number of labeled and unlabeled data\n",
        "        self._u_num = u_data[0].shape[0]\n",
        "        self._l_num = l_data[0].shape[0]\n",
        "        self._data_num = self._u_num + self._l_num\n",
        "        self._p_u = float(self._u_num) / float(self._u_num + self._l_num)\n",
        "        self._p_l = float(self._l_num) / float(self._u_num + self._l_num)\n",
        "        self._batch_size = batch_size\n",
        "        self._used_u_num, self._used_l_num = 0, 0\n",
        "        self._u_iterator = DataIterator(u_data, int(batch_size * self._p_u),\n",
        "                                        repeat=repeat, shuffle=shuffle,\n",
        "                                        epoch_finished=epoch_finished,\n",
        "                                        max_epoch=max_epoch)\n",
        "        self._l_iterator = DataIterator(l_data, int(batch_size * self._p_l),\n",
        "                                        repeat=repeat, shuffle=shuffle,\n",
        "                                        epoch_finished=epoch_finished,\n",
        "                                        max_epoch=max_epoch)\n",
        "        self._finished_epoch = epoch_finished\n",
        "        self._max_epoch = max_epoch\n",
        "        self._repeat = repeat\n",
        "        self._shuffle = shuffle\n",
        "        #这是迭代次数吗？\n",
        "        self._len = int(self._data_num / self._batch_size)\n",
        "\n",
        "    @property\n",
        "    def epoch_finished(self):\n",
        "        return self._finished_epoch\n",
        "\n",
        "    @property\n",
        "    def num_data(self):\n",
        "        return self._data_num\n",
        "\n",
        "    @property\n",
        "    def finished(self):\n",
        "        if self._max_epoch is not None:\n",
        "            return self.epoch_finished > self._max_epoch\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def __next__(self):\n",
        "        used_num = self._used_l_num + self._used_u_num + self._batch_size\n",
        "        #round()是python自带的一个函数，用于数字的四舍五入\n",
        "        next_u_num = round(used_num * self._p_u - self._used_u_num)\n",
        "        self._used_u_num += next_u_num\n",
        "        next_l_num = round(used_num * self._p_l - self._used_l_num)\n",
        "        next_l_num += self._batch_size - next_u_num - next_l_num\n",
        "        self._used_l_num += next_l_num\n",
        "        # Whatever the case, at least one sample is expected from each iterator\n",
        "        # (though the iterator may be empty, when self._repeat == False).\n",
        "        assert next_l_num != 0 and next_u_num != 0\n",
        "\n",
        "        if self._max_epoch is not None:\n",
        "            if self._finished_epoch >= self._max_epoch:\n",
        "                raise StopIteration()\n",
        "        # Stop iteration only if both iterator is finished. So no data will\n",
        "        # be missed.\n",
        "        if self._u_iterator.finished and self._l_iterator.finished:\n",
        "            raise StopIteration()\n",
        "        #用except捕捉异常StopIteration，然后令u_data=0\n",
        "        try:\n",
        "            u_data = self._u_iterator.get_next_batch(int(next_u_num))\n",
        "        except StopIteration:\n",
        "            u_data = None\n",
        "\n",
        "        try:\n",
        "            l_data = self._l_iterator.get_next_batch(int(next_l_num))\n",
        "        except StopIteration:\n",
        "            l_data = None\n",
        "\n",
        "        if not self._repeat:\n",
        "\n",
        "            # It is guaranteed here that, if one of the iterator is finished,\n",
        "            # another one will make up the missing part.\n",
        "            if self._u_iterator.finished and not self._l_iterator.finished:\n",
        "                u_num = 0 if u_data is None else u_data[0].shape[0]\n",
        "                left = self._l_iterator.get_next_batch(int(next_u_num - u_num))\n",
        "                l_data = [np.concatenate((l_data[i], left[i]))\n",
        "                          for i in range(len(l_data))]\n",
        "            if self._l_iterator.finished and not self._u_iterator.finished:\n",
        "                l_num = 0 if l_data is None else l_data[0].shape[0]\n",
        "                left = self._u_iterator.get_next_batch(int(next_l_num - l_num))\n",
        "                u_data = [np.concatenate((u_data[i], left[i]))\n",
        "                          for i in range(len(u_data))]\n",
        "\n",
        "        self._finished_epoch = min(self._u_iterator.epoch_finished,\n",
        "                                   self._l_iterator.epoch_finished)\n",
        "        if u_data is None:\n",
        "            return l_data\n",
        "        elif l_data is None:\n",
        "            return u_data\n",
        "        else:\n",
        "            # print(len(l_data))\n",
        "            return [np.concatenate((u_data[i], l_data[i]))\n",
        "                    for i in range(len(l_data))]\n",
        "            # return [(u_data[i], l_data[i])\n",
        "            # for i in range(len(l_data))]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "\n",
        "class PuLearningDataSet(object):\n",
        "    def __init__(self, cfg):\n",
        "        self._cfg = cfg\n",
        "        self._num_labeled = int(self._cfg['num_labeled'])\n",
        "        self.__num_unlabeled = None\n",
        "        self._overlap = to_bool(self._cfg['overlap'])\n",
        "        self._max_epoch = int(self._cfg['max_epoch'])\n",
        "        self._batch_size = int(self._cfg['batch_size'])\n",
        "        self._prior = None\n",
        "        #shuffle在机器学习与深度学习中代表的意思是，将训练模型的数据集进行打乱的操作\n",
        "        self._shuffled_indexes = None  # shuffle indexes for positive-unlabeled division.\n",
        "        self._negative = 0\n",
        "        self._positive = 1\n",
        "\n",
        "        self._unlabeled_positive_mask = None\n",
        "        self._unlabeled_negative_mask = None\n",
        "        self._labeled_positive_mask = None\n",
        "        self._labeled_negative_mask = None\n",
        "\n",
        "        # for initialize member variables.\n",
        "        self._prepare_pu_training_data()\n",
        "    # @property是python的一种装饰器，是用来修饰方法的，我们可以使用@property装饰器来创建只读属性，\n",
        "    # @property装饰器会将方法转换为相同名称的只读属性,可以与所定义的属性配合使用，这样可以防止属性被修改\n",
        "    @property\n",
        "    def _num_unlabeled(self):\n",
        "        assert self.__num_unlabeled is not None\n",
        "        return self.__num_unlabeled\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return self._batch_size\n",
        "\n",
        "    def _prepare_pu_training_data(self):\n",
        "        positive = self._positive\n",
        "        negative = self._negative\n",
        "        train_y = self._original_train_y()\n",
        "        train_x = self._original_train_x()\n",
        "        if self._shuffled_indexes is None:\n",
        "            #创建一个0-len（标签）长度的数组\n",
        "            self._shuffled_indexes = np.array(range(len(train_y)))\n",
        "            #通过 random 静态对象调用该方法，shuffle直接在原来的数组上进行操作，改变原来数组的顺序，无返回值\n",
        "            np.random.shuffle(self._shuffled_indexes)\n",
        "        assert self._shuffled_indexes.shape[0] == len(train_y)\n",
        "        train_y = train_y[self._shuffled_indexes]\n",
        "        train_x = train_x[self._shuffled_indexes]\n",
        "        #copy.deepcopy () 函数是一个深复制函数。 所谓深复制 ，就是从输入变量完全复刻一个相同的变量，无论怎么改变新变量，原有变量的值都 不会受到影响\n",
        "        true_y = copy.deepcopy(train_y)\n",
        "        num_pos = (train_y == positive).sum()\n",
        "        num_neg = (train_y == negative).sum()\n",
        "        #如何优交集\n",
        "        if self._overlap:\n",
        "            self.__num_unlabeled = num_neg + num_pos\n",
        "            self._prior = float(num_pos) / float(num_neg + num_pos)\n",
        "\n",
        "            #mask to index, samples with a number of _num_labeled is selected\n",
        "            #创建一个掩码，通过切片操作到1000\n",
        "            overlapped_indexes = mask_to_index(\n",
        "                train_y == positive\n",
        "            )[:self._num_labeled]\n",
        "\n",
        "            train_x_labeled = train_x[overlapped_indexes]\n",
        "            true_y_labeled = true_y[overlapped_indexes]\n",
        "            train_x = np.concatenate((train_x, train_x_labeled), axis=0)\n",
        "            true_y = np.concatenate((true_y, true_y_labeled), axis=0)\n",
        "            train_y = np.concatenate(\n",
        "                (negative * np.ones(train_y.shape), np.ones(self._num_labeled)),\n",
        "            )\n",
        "        else:\n",
        "            self.__num_unlabeled = num_neg + num_pos - self._num_labeled\n",
        "            self._prior = \\\n",
        "                float(num_pos - self._num_labeled) / \\\n",
        "                float(num_neg + num_pos - self._num_labeled)\n",
        "            train_y[train_y == positive][self._num_labeled:] = negative\n",
        "        #train_y == negative denote for unlabeled data, while true_y == positive denote for positive samples,thus and operation\n",
        "        self._unlabeled_positive_mask = np.logical_and(train_y == negative,\n",
        "                                                       true_y == positive)\n",
        "        self._unlabeled_negative_mask = np.logical_and(train_y == negative,\n",
        "                                                       true_y == negative)\n",
        "        self._labeled_positive_mask = np.logical_and(train_y == positive,\n",
        "                                                     true_y == positive)\n",
        "        self._labeled_negative_mask = np.logical_and(train_y == positive,\n",
        "                                                     true_y == negative)\n",
        "        return train_x, train_y\n",
        "\n",
        "    def _prepare_pn_testing_data(self):\n",
        "        #just return original test data as the pn test data\n",
        "        test_y = self._original_test_y()\n",
        "        test_x = self._original_test_x()\n",
        "        #把test_y调成1列的结构\n",
        "        test_y = test_y.reshape([-1, 1])\n",
        "        return test_x, test_y\n",
        "\n",
        "    def get_training_iterator(self, batch_size=None, repeat=True, shuffle=True,\n",
        "                              max_epoch=None):\n",
        "        x, y = self._prepare_pu_training_data()\n",
        "        if max_epoch is None:\n",
        "            max_epoch = self._max_epoch\n",
        "        if batch_size is None:\n",
        "            batch_size = self._batch_size\n",
        "        y = y.reshape([-1, 1])\n",
        "        #这是一个或的结构，self._unlabeled_positive_mask和self._unlabeled_negative_mask里真的元素加到一起\n",
        "        unlabeled_mask = np.logical_or(self._unlabeled_positive_mask,\n",
        "                                       self._unlabeled_negative_mask)\n",
        "        u_x = x[unlabeled_mask]\n",
        "        u_y = y[unlabeled_mask]\n",
        "\n",
        "        labeled_mask = np.logical_or(self._labeled_positive_mask,\n",
        "                                     self._labeled_negative_mask)\n",
        "        l_x = x[labeled_mask]\n",
        "        l_y = y[labeled_mask]\n",
        "\n",
        "        return PuDataIterator((u_x, u_y),\n",
        "                              (l_x, l_y),\n",
        "                              batch_size, max_epoch=max_epoch, repeat=repeat,\n",
        "                              shuffle=shuffle)\n",
        "\n",
        "    def get_testing_iterator(self, batch_size=None):\n",
        "        x, y = self._prepare_pn_testing_data()\n",
        "        if batch_size is None:\n",
        "            batch_size = \\\n",
        "                self._batch_size if self._batch_size <= len(y) else len(y)\n",
        "        return DataIterator((x, y), batch_size, max_epoch=1, repeat=False,\n",
        "                            shuffle=False)\n",
        "\n",
        "    @property\n",
        "    def prior(self):\n",
        "        assert self._prior is not None\n",
        "        return self._prior\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _original_train_x(self):\n",
        "        pass\n",
        "    # 我们抽象出一个基类，知道要有哪些方法，但只是抽象方法，并不实现功能，只能继承，而不能被实例化，\n",
        "    # 但子类必须要实现该方法，这就需要用到抽象基类\n",
        "    @abc.abstractmethod\n",
        "    def _original_train_y(self):\n",
        "        \"\"\"\n",
        "        Should return  binarized labels.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _original_test_x(self):\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _original_test_y(self):\n",
        "        \"\"\"\n",
        "        Should return  binarized labels.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "class PnLearningDataSet(object):\n",
        "    def __init__(self, cfg, prior):\n",
        "        self._cfg = cfg\n",
        "        self._max_epoch = int(self._cfg['max_epoch'])\n",
        "        self._batch_size = int(self._cfg['batch_size'])\n",
        "        self._prior = prior\n",
        "        self._shuffled_indexes = None  # shuffle indexes for positive-unlabeled division.\n",
        "        if prior is None:\n",
        "            self._num_pos = None\n",
        "            self._num_neg = None\n",
        "        else:\n",
        "            self._num_pos = int(self._cfg['num_pos'])\n",
        "            self._num_neg = int(math.pow((1 - prior) / (2 * prior), 2) * self._num_pos)\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return self._batch_size\n",
        "\n",
        "    def _prepare_pn_training_data(self):\n",
        "        positive = 1\n",
        "        negative = -1\n",
        "        train_y = self._original_train_y()\n",
        "        train_x = self._original_train_x()\n",
        "        if self._shuffled_indexes is None:\n",
        "            self._shuffled_indexes = list(range(len(train_y)))\n",
        "            np.random.shuffle(self._shuffled_indexes)\n",
        "        train_y = train_y[self._shuffled_indexes]\n",
        "        train_x = train_x[self._shuffled_indexes]\n",
        "        num_pos = (train_y == positive).sum()\n",
        "        num_neg = (train_y == negative).sum()\n",
        "        if self._num_neg is None:\n",
        "            self._num_neg = num_neg\n",
        "        if self._num_pos is None:\n",
        "            self._num_pos = num_pos\n",
        "        assert num_pos >= self._num_pos and num_neg >= self._num_neg\n",
        "        train_x = np.concatenate((train_x[train_y == positive][:self._num_pos],\n",
        "                                 train_x[train_y == negative][:self._num_neg]))\n",
        "        train_y = np.concatenate((train_y[train_y == positive][:self._num_pos],\n",
        "                                 train_y[train_y == negative][:self._num_neg]))\n",
        "        train_y = train_y.reshape([-1, 1])\n",
        "        return train_x, train_y\n",
        "\n",
        "    def _prepare_pn_testing_data(self):\n",
        "        test_y = self._original_test_y()\n",
        "        test_x = self._original_test_x()\n",
        "        test_y = test_y.reshape([-1, 1])\n",
        "        return test_x, test_y\n",
        "\n",
        "    def get_training_iterator(self, batch_size=None, repeat=True, shuffle=True,\n",
        "                              max_epoch=None):\n",
        "        x, y = self._prepare_pn_training_data()\n",
        "        if max_epoch is None:\n",
        "            max_epoch = self._max_epoch\n",
        "        if batch_size is None:\n",
        "            batch_size = \\\n",
        "                self._batch_size if self._batch_size <= len(y) else len(y)\n",
        "        return DataIterator((x, y), batch_size, max_epoch=max_epoch,\n",
        "                            repeat=repeat, shuffle=shuffle)\n",
        "\n",
        "    def get_testing_iterator(self, batch_size=None):\n",
        "        x, y = self._prepare_pn_testing_data()\n",
        "        if batch_size is None:\n",
        "            batch_size = \\\n",
        "                self._batch_size if self._batch_size <= len(y) else len(y)\n",
        "        return DataIterator((x, y), batch_size, max_epoch=1, repeat=False,\n",
        "                            shuffle=False)\n",
        "\n",
        "    @property\n",
        "    def prior(self):\n",
        "        assert self._prior is not None\n",
        "        return self._prior\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _original_train_x(self):\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _original_train_y(self):\n",
        "        \"\"\"\n",
        "        Should return  binarized labels.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _original_test_x(self):\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def _original_test_y(self):\n",
        "        \"\"\"\n",
        "        Should return  binarized labels.\n",
        "        \"\"\"\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTEnQlPH0Fcs"
      },
      "source": [
        "引入数据集并进行处理"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4IDVnRiUbTs"
      },
      "outputs": [],
      "source": [
        "!pip install medmnist\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from medmnist.info import INFO, HOMEPAGE, DEFAULT_ROOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_kB5lvCUheS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from medmnist.info import INFO, HOMEPAGE, DEFAULT_ROOT\n",
        "\n",
        "\n",
        "class MedMNIST(Dataset):\n",
        "\n",
        "    flag = ...\n",
        "\n",
        "    def __init__(self,\n",
        "                 split,\n",
        "                 transform=None,\n",
        "                 target_transform=None,\n",
        "                 download=False,\n",
        "                 as_rgb=False,\n",
        "                 root=DEFAULT_ROOT):\n",
        "        ''' dataset\n",
        "        :param split: 'train', 'val' or 'test', select subset\n",
        "        :param transform: data transformation\n",
        "        :param target_transform: target transformation\n",
        "        '''\n",
        "\n",
        "        self.info = INFO[self.flag]\n",
        "\n",
        "        if root is not None and os.path.exists(root):\n",
        "            self.root = root\n",
        "        else:\n",
        "            raise RuntimeError(\"Failed to setup the default `root` directory. \" +\n",
        "                               \"Please specify and create the `root` directory manually.\")\n",
        "\n",
        "        if download:\n",
        "            self.download()\n",
        "\n",
        "        if not os.path.exists(\n",
        "                os.path.join(self.root, \"{}.npz\".format(self.flag))):\n",
        "            raise RuntimeError('Dataset not found. ' +\n",
        "                               ' You can set `download=True` to download it')\n",
        "\n",
        "        npz_file = np.load(os.path.join(self.root, \"{}.npz\".format(self.flag)))\n",
        "\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.as_rgb = as_rgb\n",
        "\n",
        "        if self.split == 'train':\n",
        "            self.imgs = npz_file['train_images']\n",
        "            self.labels = npz_file['train_labels']\n",
        "        elif self.split == 'val':\n",
        "            self.imgs = npz_file['val_images']\n",
        "            self.labels = npz_file['val_labels']\n",
        "        elif self.split == 'test':\n",
        "            self.imgs = npz_file['test_images']\n",
        "            self.labels = npz_file['test_labels']\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.imgs.shape[0]\n",
        "\n",
        "    def __repr__(self):\n",
        "        '''Adapted from torchvision.ss'''\n",
        "        _repr_indent = 4\n",
        "        head = f\"Dataset {self.__class__.__name__} ({self.flag})\"\n",
        "        body = [f\"Number of datapoints: {self.__len__()}\"]\n",
        "        body.append(f\"Root location: {self.root}\")\n",
        "        body.append(f\"Split: {self.split}\")\n",
        "        body.append(f\"Task: {self.info['task']}\")\n",
        "        body.append(f\"Number of channels: {self.info['n_channels']}\")\n",
        "        body.append(f\"Meaning of labels: {self.info['label']}\")\n",
        "        body.append(f\"Number of samples: {self.info['n_samples']}\")\n",
        "        body.append(f\"Description: {self.info['description']}\")\n",
        "        body.append(f\"License: {self.info['license']}\")\n",
        "\n",
        "        lines = [head] + [\" \" * _repr_indent + line for line in body]\n",
        "        return '\\n'.join(lines)\n",
        "\n",
        "    def download(self):\n",
        "        try:\n",
        "            from torchvision.datasets.utils import download_url\n",
        "            download_url(url=self.info[\"url\"],\n",
        "                         root=self.root,\n",
        "                         filename=\"{}.npz\".format(self.flag),\n",
        "                         md5=self.info[\"MD5\"])\n",
        "        except:\n",
        "            raise RuntimeError('Something went wrong when downloading! ' +\n",
        "                               'Go to the homepage to download manually. ' +\n",
        "                               HOMEPAGE)\n",
        "\n",
        "\n",
        "class MedMNIST2D(MedMNIST):\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        return: (without transform/target_transofrm)\n",
        "            img: PIL.Image\n",
        "            target: np.array of `L` (L=1 for single-label)\n",
        "        '''\n",
        "        img, target = self.imgs[index], self.labels[index].astype(int)\n",
        "        img = Image.fromarray(img)\n",
        "\n",
        "        if self.as_rgb:\n",
        "            img = img.convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def save(self, folder, postfix=\"png\", write_csv=True):\n",
        "\n",
        "        from medmnist.utils import save2d\n",
        "\n",
        "        save2d(imgs=self.imgs,\n",
        "               labels=self.labels,\n",
        "               img_folder=os.path.join(folder, self.flag),\n",
        "               split=self.split,\n",
        "               postfix=postfix,\n",
        "               csv_path=os.path.join(folder, f\"{self.flag}.csv\") if write_csv else None)\n",
        "\n",
        "    def montage(self, length=20, replace=False, save_folder=None):\n",
        "        from medmnist.utils import montage2d\n",
        "\n",
        "        n_sel = length * length\n",
        "        sel = np.random.choice(self.__len__(), size=n_sel, replace=replace)\n",
        "\n",
        "        montage_img = montage2d(imgs=self.imgs,\n",
        "                                n_channels=self.info['n_channels'],\n",
        "                                sel=sel)\n",
        "\n",
        "        if save_folder is not None:\n",
        "            if not os.path.exists(save_folder):\n",
        "                os.makedirs(save_folder)\n",
        "            montage_img.save(os.path.join(save_folder,\n",
        "                                          f\"{self.flag}_{self.split}_montage.jpg\"))\n",
        "\n",
        "        return montage_img\n",
        "\n",
        "class BreastMNIST(MedMNIST2D):\n",
        "    flag = \"breastmnist\"\n",
        "class PneumoniaMNIST(MedMNIST2D):\n",
        "    flag = \"pneumoniamnist\"\n",
        "class OCTMNIST(MedMNIST2D):\n",
        "    flag = \"octmnist\"\n",
        "class BloodMNIST(MedMNIST2D):\n",
        "    flag = \"bloodmnist\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgXKwTVUy5Zw"
      },
      "source": [
        "引入乳腺超声数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3xtP67JL9C4"
      },
      "outputs": [],
      "source": [
        "__author__ = 'garrett_local'\n",
        "\n",
        "\n",
        "def _prepare_BreastMNIST_data():\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    train_x1 = BreastMNIST(\"train\",download=True).imgs\n",
        "    train_y1 = BreastMNIST(\"train\",download=True).labels\n",
        "    train_x2 = BreastMNIST(\"val\",download=True).imgs\n",
        "    train_y2 = BreastMNIST(\"val\",download=True).labels\n",
        "    train_x = np.concatenate((train_x1,train_x2),axis=0)\n",
        "    train_y = np.concatenate((train_y1,train_y2),axis=0)\n",
        "    test_x = BreastMNIST(\"test\",download=True).imgs\n",
        "    test_y = BreastMNIST(\"test\",download=True).labels\n",
        "\n",
        "    train_x = np.reshape(np.array(train_x), (train_x.shape[0], 28, 28, 1)) / 255.\n",
        "    test_x = np.reshape(np.array(test_x), (test_x.shape[0], 28, 28, 1)) / 255.\n",
        "\n",
        "    train_x = np.asarray(train_x[:], dtype=np.float32)\n",
        "    train_y = np.asarray(train_y[:], dtype=np.int32).squeeze(1)\n",
        "\n",
        "    test_x = np.asarray(test_x[:], dtype=np.float32)\n",
        "    test_y = np.asarray(test_y[:], dtype=np.int32).squeeze(1)\n",
        "    # pdb.set_trace()\n",
        "    return train_x, train_y, test_x, test_y\n",
        "class BreastMNISTDataset(PuLearningDataSet):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._train_x, self._train_y, self._test_x, self._test_y = \\\n",
        "            _prepare_BreastMNIST_data()\n",
        "        super(BreastMNISTDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _original_train_x(self):\n",
        "        return self._train_x\n",
        "\n",
        "    def _original_train_y(self):\n",
        "        return self._train_y\n",
        "\n",
        "    def _original_test_x(self):\n",
        "        return self._test_x\n",
        "\n",
        "    def _original_test_y(self):\n",
        "        return self._test_y\n",
        "\n",
        "\n",
        "class BreastMNISTPnDataset(PnLearningDataSet):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._train_x, self._train_y, self._test_x, self._test_y = \\\n",
        "            _prepare_BreastMNIST_data()\n",
        "        super(BreastMNISTPnDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _original_train_x(self):\n",
        "        return self._train_x\n",
        "\n",
        "    def _original_train_y(self):\n",
        "        return self._train_y\n",
        "\n",
        "    def _original_test_x(self):\n",
        "        return self._test_x\n",
        "\n",
        "    def _original_test_y(self):\n",
        "        return self._test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03XRCijZzmKx"
      },
      "source": [
        "引入胸部X光数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b80XzTNzrSDo"
      },
      "outputs": [],
      "source": [
        "def _prepare_PneumoniaMNIST_data():\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    train_x1 = PneumoniaMNIST(\"train\",download=True).imgs\n",
        "    train_y1 = PneumoniaMNIST(\"train\",download=True).labels\n",
        "    train_x2 = PneumoniaMNIST(\"val\",download=True).imgs\n",
        "    train_y2 = PneumoniaMNIST(\"val\",download=True).labels\n",
        "    train_x = np.concatenate((train_x1,train_x2),axis=0)\n",
        "    train_y = np.concatenate((train_y1,train_y2),axis=0)\n",
        "    test_x = PneumoniaMNIST(\"test\",download=True).imgs\n",
        "    test_y = PneumoniaMNIST(\"test\",download=True).labels\n",
        "\n",
        "    train_x = np.reshape(np.array(train_x), (train_x.shape[0], 28, 28, 1)) / 255.\n",
        "    test_x = np.reshape(np.array(test_x), (test_x.shape[0], 28, 28, 1)) / 255.\n",
        "\n",
        "    train_x = np.asarray(train_x[:], dtype=np.float32)\n",
        "    train_y = np.asarray(train_y[:], dtype=np.int32).squeeze(1)\n",
        "\n",
        "    test_x = np.asarray(test_x[:], dtype=np.float32)\n",
        "    test_y = np.asarray(test_y[:], dtype=np.int32).squeeze(1)\n",
        "    # pdb.set_trace()\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "class PneumoniaMNISTDataset(PuLearningDataSet):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._train_x, self._train_y, self._test_x, self._test_y = \\\n",
        "            _prepare_PneumoniaMNIST_data()\n",
        "        super(PneumoniaMNISTDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _original_train_x(self):\n",
        "        return self._train_x\n",
        "\n",
        "    def _original_train_y(self):\n",
        "        return self._train_y\n",
        "\n",
        "    def _original_test_x(self):\n",
        "        return self._test_x\n",
        "\n",
        "    def _original_test_y(self):\n",
        "        return self._test_y\n",
        "\n",
        "\n",
        "class PneumoniaMNISTPnDataset(PnLearningDataSet):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._train_x, self._train_y, self._test_x, self._test_y = \\\n",
        "            _prepare_PneumoniaMNIST_data()\n",
        "        super(PneumoniaMNISTPnDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _original_train_x(self):\n",
        "        return self._train_x\n",
        "\n",
        "    def _original_train_y(self):\n",
        "        return self._train_y\n",
        "\n",
        "    def _original_test_x(self):\n",
        "        return self._test_x\n",
        "\n",
        "    def _original_test_y(self):\n",
        "        return self._test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTVI9GvmzpFj"
      },
      "source": [
        "引入OCT数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcKxJG_J1RZ7"
      },
      "outputs": [],
      "source": [
        "def _prepare_OCTMNIST_data():\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    train_x1 = OCTMNIST(\"train\",download=True).imgs\n",
        "    train_y1 = OCTMNIST(\"train\",download=True).labels\n",
        "    train_x2 = OCTMNIST(\"val\",download=True).imgs\n",
        "    train_y2 = OCTMNIST(\"val\",download=True).labels\n",
        "    train_x = np.concatenate((train_x1,train_x2),axis=0)\n",
        "    train_y = np.concatenate((train_y1,train_y2),axis=0)\n",
        "    test_x = OCTMNIST(\"test\",download=True).imgs\n",
        "    test_y = OCTMNIST(\"test\",download=True).labels\n",
        "\n",
        "    train_x = np.reshape(np.array(train_x), (train_x.shape[0], 28, 28, 1)) / 255.\n",
        "    test_x = np.reshape(np.array(test_x), (test_x.shape[0], 28, 28, 1)) / 255.\n",
        "\n",
        "    train_x = np.asarray(train_x[:], dtype=np.float32)\n",
        "    train_y = np.asarray(train_y[:], dtype=np.int32).squeeze(1)\n",
        "\n",
        "    test_x = np.asarray(test_x[:], dtype=np.float32)\n",
        "    test_y = np.asarray(test_y[:], dtype=np.int32).squeeze(1)\n",
        "\n",
        "    # Binarize labels.\n",
        "    train_y[train_y % 2 == 1] = -1\n",
        "    train_y[train_y % 2 == 0] = 1\n",
        "    train_y[train_y == -1] = 0\n",
        "    test_y[test_y % 2 == 1] = -1\n",
        "    test_y[test_y % 2 == 0] = 1\n",
        "    test_y[test_y == -1] = 0\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "class OCTMNISTDataset(PuLearningDataSet):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._train_x, self._train_y, self._test_x, self._test_y = \\\n",
        "            _prepare_OCTMNIST_data()\n",
        "        super(OCTMNISTDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _original_train_x(self):\n",
        "        return self._train_x\n",
        "\n",
        "    def _original_train_y(self):\n",
        "        return self._train_y\n",
        "\n",
        "    def _original_test_x(self):\n",
        "        return self._test_x\n",
        "\n",
        "    def _original_test_y(self):\n",
        "        return self._test_y\n",
        "\n",
        "\n",
        "class OCTMNISTPnDataset(PnLearningDataSet):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._train_x, self._train_y, self._test_x, self._test_y = \\\n",
        "            _prepare_OCTMNIST_data()\n",
        "        super(OCTMNISTPnDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _original_train_x(self):\n",
        "        return self._train_x\n",
        "\n",
        "    def _original_train_y(self):\n",
        "        return self._train_y\n",
        "\n",
        "    def _original_test_x(self):\n",
        "        return self._test_x\n",
        "\n",
        "    def _original_test_y(self):\n",
        "        return self._test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s8cgPOwMWrn"
      },
      "source": [
        "引入BloodMNIST数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y81Kxs-dMWMM"
      },
      "outputs": [],
      "source": [
        "def _prepare_BloodMNIST_data():\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    train_x1 = BloodMNIST(\"train\",download=True).imgs\n",
        "    train_y1 = BloodMNIST(\"train\",download=True).labels\n",
        "    train_x2 = BloodMNIST(\"val\",download=True).imgs\n",
        "    train_y2 = BloodMNIST(\"val\",download=True).labels\n",
        "    train_x = np.concatenate((train_x1,train_x2),axis=0)\n",
        "    train_y = np.concatenate((train_y1,train_y2),axis=0)\n",
        "    test_x = BloodMNIST(\"test\",download=True).imgs\n",
        "    test_y = BloodMNIST(\"test\",download=True).labels\n",
        "\n",
        "    train_x = train_x.reshape((train_x.shape[0], 3, 28, 28)) / 255.0 #transpose([0, 2, 3, 1])\n",
        "    test_x = test_x.reshape((test_x.shape[0], 3, 28, 28)) / 255.0 #.transpose([0, 2, 3, 1])\n",
        "\n",
        "    train_x = np.asarray(train_x[:], dtype=np.float32)\n",
        "    train_y = np.asarray(train_y[:], dtype=np.int32).squeeze(1)\n",
        "\n",
        "    test_x = np.asarray(test_x[:], dtype=np.float32)\n",
        "    test_y = np.asarray(test_y[:], dtype=np.int32).squeeze(1)\n",
        "\n",
        "    # Binarize labels.\n",
        "    train_y[train_y % 2 == 1] = -1\n",
        "    train_y[train_y % 2 == 0] = 1\n",
        "    train_y[train_y == -1] = 0\n",
        "    test_y[test_y % 2 == 1] = -1\n",
        "    test_y[test_y % 2 == 0] = 1\n",
        "    test_y[test_y == -1] = 0\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "class BloodMNISTDataset(PuLearningDataSet):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._train_x, self._train_y, self._test_x, self._test_y = \\\n",
        "            _prepare_BloodMNIST_data()\n",
        "        super(BloodMNISTDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _original_train_x(self):\n",
        "        return self._train_x\n",
        "\n",
        "    def _original_train_y(self):\n",
        "        return self._train_y\n",
        "\n",
        "    def _original_test_x(self):\n",
        "        return self._test_x\n",
        "\n",
        "    def _original_test_y(self):\n",
        "        return self._test_y\n",
        "\n",
        "\n",
        "class BloodMNISTPnDataset(PnLearningDataSet):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._train_x, self._train_y, self._test_x, self._test_y = \\\n",
        "            _prepare_BloodMNIST_data()\n",
        "        super(BloodMNISTPnDataset, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def _original_train_x(self):\n",
        "        return self._train_x\n",
        "\n",
        "    def _original_train_y(self):\n",
        "        return self._train_y\n",
        "\n",
        "    def _original_test_x(self):\n",
        "        return self._test_x\n",
        "\n",
        "    def _original_test_y(self):\n",
        "        return self._test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXCaAjARs4Q6"
      },
      "outputs": [],
      "source": [
        "#from helper import cifar10_dataset\n",
        "#from helper import mnist_dataset\n",
        "#from helper import sddd_dataset\n",
        "#from helper import news_dataset\n",
        "# import network\n",
        "\n",
        "__author__ = 'garrett_local'\n",
        "\n",
        "\n",
        "def load_dataset(cfg):\n",
        "    dataset_name = cfg['dataset']['dataset_name']\n",
        "\n",
        "    if dataset_name == 'BreastMNIST':\n",
        "        return BreastMNISTDataset, BreastMNISTPnDataset\n",
        "    if dataset_name == 'PneumoniaMNIST':\n",
        "        return PneumoniaMNISTDataset, PneumoniaMNISTPnDataset\n",
        "    if dataset_name == 'OCTMNIST':\n",
        "        return OCTMNISTDataset, OCTMNISTPnDataset\n",
        "    if dataset_name == 'BloodMNIST':\n",
        "        return BloodMNISTDataset, BloodMNISTPnDataset\n",
        "    if dataset_name == 'AMD':\n",
        "        return AMDDataset, AMDPnDataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxT1p8Ct2cIK"
      },
      "source": [
        "from helper import ploting_helper 失败，复制源代码"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW3bNiy6-l59"
      },
      "source": [
        "引入模块库\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpRwNm1L-nLL"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzM_zS66-3Oj"
      },
      "source": [
        "设置参数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZlVARH3Opmp"
      },
      "source": [
        "parser.add_argument('--model', type=str, default='pan', choices=['pan', 'nnpu', 'upu', 'agan'],\n",
        "                    help='Please select model that you want to run.')\n",
        "parser.add_argument('--dataset', type=str, default='cifar10', choices=['mnist', 'cifar10', 'sddd', 'news'],\n",
        "                    help='Using dataset.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpGygpp9rEQ9"
      },
      "outputs": [],
      "source": [
        "class Argument():\n",
        "    def __init__(self):\n",
        "      self.n_epochs=200\n",
        "      self.batch_size=64\n",
        "      self.dropoutrate=0.3\n",
        "      self.lr=0.0001\n",
        "      self.b1=0.5\n",
        "      self.b2=0.999\n",
        "      self.n_cpu=8\n",
        "      self.latent_dim=100\n",
        "      self.img_size=28\n",
        "      self.channels=1\n",
        "      self.sample_interval=400\n",
        "      self.gpu=0\n",
        "      self.model='pan'\n",
        "      self.dataset='BloodMNIST'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyTquLAu-2oh"
      },
      "outputs": [],
      "source": [
        "os.makedirs('images', exist_ok=True)\n",
        "opt = Argument()\n",
        "#print(opt.channels, opt.img_size, opt.img_size)\n",
        "model_style = {'BreastMNIST': 'mlp', 'PneumoniaMNIST': 'mlp', 'OCTMNIST': 'mlp', 'BloodMNIST': 'conv','AMD': 'conv'}\n",
        "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
        "img_shape = (1, 28, 28)\n",
        "#用默认的CPU\n",
        "torch.cuda.set_device(0)\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "#给定学习率，目标函数，\n",
        "ratios = [0.8, 0.7, 0.6, 0.5, 0.4]\n",
        "#目标函数  KL散度\n",
        "optimize_type = \"holder\"\n",
        "loss_type = \"3rd\"\n",
        "# ratio = 0.6\n",
        "#ratios = [0.5,0.4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0mJdONyXaLb"
      },
      "source": [
        "主函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOhT2HsLXUN3"
      },
      "outputs": [],
      "source": [
        "for ratio in ratios:\n",
        "    def adjust_lr(optimizer, epoch):\n",
        "        lr = opt.lr\n",
        "        #“//”:取整除 - 返回商的整数部分（向下取整）\n",
        "        # “**”:幂运算\n",
        "        lr1 = opt.lr * (0.5 ** (epoch // 20))\n",
        "        if lr == lr1:\n",
        "            return\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr1\n",
        "\n",
        "\n",
        "    class Discriminator_MLP(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Discriminator_MLP, self).__init__()\n",
        "            # dense1_bn = nn.BatchNorm1d(512)\n",
        "            # dense2_bn = nn.BatchNorm1d(256)\n",
        "            #创建一个MLP全连接模型，输入是784个像素点，一共10层,\n",
        "            #第一层也就是序号为0的层数，输入是784个像素点，输出是512个像素点\n",
        "            #第二层是归一化处理，BatchNorm通过对加入参数后的数据进行归一处理，来加快收敛速度和防止过拟合\n",
        "            #第三层是为了解决Dead ReLU问题的激活函数层\n",
        "            #第四次是Dropout，是用来防止过拟合的。\n",
        "            # nn.Dropout(p = 0.3) 表示每个神经元有0.3的可能性不被激活\n",
        "            self.model = nn.Sequential(\n",
        "                nn.Linear(int(np.prod(img_shape)), 512),\n",
        "                nn.BatchNorm1d(512),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.BatchNorm1d(256),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                nn.Linear(256, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "        def forward(self, img):\n",
        "            #.view(x.size(0), -1)这句话是说将第二次卷积的输出拉伸为一行，这句代码中的上一句中x的执行结果为：50*32*7*7个数\n",
        "            # 其中，50代表的是批次训练是选取的批量数BATCH_SIZE，每次选择50个数进行训练。\n",
        "            # 32代表的是out_channels，7*7代表的是每张图像处理之后的尺度。\n",
        "            img_flat = img.view(img.size(0), -1)\n",
        "            validity = self.model(img_flat)\n",
        "\n",
        "            return validity\n",
        "\n",
        "\n",
        "    class Recognizer_MLP(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Recognizer_MLP, self).__init__()\n",
        "            # dense1_bn = nn.BatchNorm1d(512)\n",
        "            # dense2_bn = nn.BatchNorm1d(256)\n",
        "\n",
        "            self.model = nn.Sequential(\n",
        "                nn.Linear(int(np.prod(img_shape)), 512),\n",
        "                nn.BatchNorm1d(512),\n",
        "                # nn.ReLU(),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                nn.Linear(512, 256),\n",
        "                nn.BatchNorm1d(256),\n",
        "                # nn.ReLU(),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                nn.Linear(256, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "        def forward(self, img):\n",
        "            img_flat = img.view(img.size(0), -1)\n",
        "            validity = self.model(img_flat)\n",
        "\n",
        "            return validity\n",
        "\n",
        "\n",
        "    class Recognizer_CNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Recognizer_CNN, self).__init__()\n",
        "            # dense1_bn = nn.BatchNorm1d(512)\n",
        "            # dense2_bn = nn.BatchNorm1d(256)\n",
        "            #创建一个15层的卷积神经网络\n",
        "            #第一层是输出频道数为3，输入频道数为96，卷积核是3*3，其他为默认值\n",
        "            #输入32*32的RGB格式的图像，输出的是这个尺寸:torch.Size([1, 10, 14, 14])\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(3, 84, 3),\n",
        "                nn.BatchNorm2d(84),\n",
        "                nn.ReLU(),\n",
        "                # nn.BatchNorm2d(84),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                # nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(84, 84, 3, stride=2),\n",
        "                nn.BatchNorm2d(84),\n",
        "                nn.ReLU(),\n",
        "                # nn.BatchNorm2d(84),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                # nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(84, 168, 1),\n",
        "                nn.BatchNorm2d(168),\n",
        "                nn.ReLU(),\n",
        "                # nn.BatchNorm2d(168),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                # nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(168, 8, 1),\n",
        "                nn.ReLU(),\n",
        "                # nn.BatchNorm2d(8),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                # nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "\n",
        "            self.fc1 = nn.Sequential(\n",
        "                nn.Linear(144 * 8, 1000),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                # nn.Dropout(opt.dropoutrate),\n",
        "                nn.Linear(1000, 1),\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "\n",
        "        def forward(self, img):\n",
        "            conv_d = self.conv(img)\n",
        "            out = self.fc1(conv_d.view(conv_d.shape[0], -1))\n",
        "\n",
        "            return out\n",
        "\n",
        "\n",
        "    class Discriminator_CNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(Discriminator_CNN, self).__init__()\n",
        "            # dense1_bn = nn.BatchNorm1d(512)\n",
        "            # dense2_bn = nn.BatchNorm1d(256)\n",
        "\n",
        "            self.conv = nn.Sequential(\n",
        "                nn.Conv2d(3, 84, 3),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(84),\n",
        "                # nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                nn.Conv2d(84, 84, 3, stride=2),\n",
        "                # nn.BatchNorm2d(84),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(84),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                nn.Conv2d(84, 168, 1),\n",
        "                nn.ReLU(),\n",
        "                nn.BatchNorm2d(168),\n",
        "                nn.Dropout(opt.dropoutrate),\n",
        "                # nn.BatchNorm2d(84),\n",
        "                # nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Conv2d(168, 8, 1),\n",
        "                nn.ReLU(),\n",
        "                # nn.BatchNorm2d(8),\n",
        "                # nn.LeakyReLU(0.2, inplace=True),\n",
        "            )\n",
        "\n",
        "            self.fc1 = nn.Sequential(\n",
        "                nn.Linear(144 * 8, 1000),\n",
        "                # nn.ReLU(),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                # nn.Dropout(opt.dropoutrate),\n",
        "                nn.Linear(1000, 1),\n",
        "                nn.Sigmoid(),\n",
        "            )\n",
        "\n",
        "        def forward(self, img):\n",
        "            conv_d = self.conv(img)\n",
        "            #view 重新定义矩阵形式\n",
        "            out = self.fc1(conv_d.view(conv_d.shape[0], -1))\n",
        "\n",
        "            return out\n",
        "\n",
        "    #将得分结果传入这个模型里，将得到的索引转换为标签\n",
        "    def index2label(index):\n",
        "        return 2 * index.float() - 1\n",
        "\n",
        "\n",
        "    def rl(score_r):\n",
        "        ###score_r : 300*1\n",
        "        p_pos, p_neg = score_r, 1 - score_r\n",
        "        # torch.cat的功能是将多个tensor类型矩阵的连接。它有两个参数，\n",
        "        # 第一个是tensor元组或者tensor列表；第二个是dim，如果tensor是二维的，dim=0指在行上连接，dim=1指在列上连接\n",
        "        p = torch.cat((p_neg, p_pos), 1)\n",
        "        #### p : 300*2\n",
        "        #输出形状中，将dim维设定为1，其它与输入形状保持一致。下面以列的形式输出p 中每行最大的元素\n",
        "        #返回最大值，最大值的索引（从0开始）\n",
        "        max_prob, max_index = torch.max(p, 1)\n",
        "        ####max_prob : 300*1\n",
        "        ####max_index : 300*1 (0/1)\n",
        "        #float()将数据转换为浮点型\n",
        "        #unsqueeze(1)返回一个新的张量，对输入的既定位置插入维度 1\n",
        "        normal_index = max_index.float().unsqueeze(1)\n",
        "        reverse_index = 1.0 - normal_index\n",
        "        action_index = torch.cat((reverse_index, normal_index), 1)\n",
        "\n",
        "        neg_log_prob = torch.log(max_prob)\n",
        "\n",
        "        ###change to nll\n",
        "        label = index2label(max_index)\n",
        "        ####label: 300*1 (-1/1)\n",
        "        # print('mmm',max_prob)\n",
        "        # print('label',label)\n",
        "        max_prob = max_prob * label\n",
        "        # print('max_prob',max_prob.size())\n",
        "        return max_prob.unsqueeze(1), action_index\n",
        "\n",
        "    #通过format函数，找到对应的数据集文件\n",
        "    cfg_path = './cfg/{}'.format(opt.dataset)\n",
        "    #os.path.join 用于拼接路径，读取文件数，下面三个文件可以看做参数文件\n",
        "    pn_cfg = read_cfg_file(os.path.join(cfg_path, 'PN'))\n",
        "    upu_cfg = read_cfg_file(os.path.join(cfg_path, 'uPU'))\n",
        "    nnpu_cfg = read_cfg_file(os.path.join(cfg_path, 'nnPU'))\n",
        "    #这是一个检验函数，检验文件里的参数是否正确\n",
        "    #get_unique_name()返回的是'{}'.format(int(time.time()))，返回当前时间\n",
        "    assert \\\n",
        "        upu_cfg['dataset']['dataset_name'] == \\\n",
        "        nnpu_cfg['dataset']['dataset_name'] and \\\n",
        "        upu_cfg['network']['network_name'] == \\\n",
        "        nnpu_cfg['network']['network_name'] and \\\n",
        "        pn_cfg['dataset']['dataset_name'] == \\\n",
        "        nnpu_cfg['dataset']['dataset_name'] and \\\n",
        "        pn_cfg['network']['network_name'] == \\\n",
        "        nnpu_cfg['network']['network_name']\n",
        "    exp_name = 'exp_{}_{}_{}'.format(\n",
        "        nnpu_cfg['dataset']['dataset_name'],\n",
        "        nnpu_cfg['network']['network_name'],\n",
        "        get_unique_name()\n",
        "    )\n",
        "    #它实例化了一个叫LogData的类，位于helper文件夹的file_helper文件的最下面\n",
        "    log_data = LogData()\n",
        "\n",
        "    # upu and nnpu.\n",
        "    # load_dataset 位于helper文件夹的training_helper文件\n",
        "    # 实例化两个类，这两个类中的属性是处理后的数据集数据（归一化处理后的训练数据和测试数据）\n",
        "    # 在这两个类中，下载了数据集并对数据进行了简单处理，以下三行信息量极大\n",
        "    #完成了迭代器的初始化，挑选出了第一批训练数据和测试数据\n",
        "    PuDataset, PnDataset = load_dataset(upu_cfg)\n",
        "    pu_dataset = PuDataset(upu_cfg['dataset'])\n",
        "    training_iterator = pu_dataset.get_training_iterator()\n",
        "\n",
        "    # Loss function\n",
        "    adversarial_loss = torch.nn.BCELoss()\n",
        "\n",
        "    # Initialize generator and discriminator\n",
        "    # generator = Generator()\n",
        "    if model_style[opt.dataset] == 'mlp':\n",
        "        discriminator = Discriminator_MLP()\n",
        "        recognizer = Recognizer_MLP()\n",
        "    else:\n",
        "        discriminator = Discriminator_CNN()\n",
        "        recognizer = Recognizer_CNN()\n",
        "\n",
        "    if cuda:\n",
        "        # generator.cuda()\n",
        "        discriminator.cuda()\n",
        "        recognizer.cuda()\n",
        "        # adversarial_loss.cuda()\n",
        "\n",
        "    # Optimizers\n",
        "    # optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
        "    optimizer = \"Adam\"\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2), weight_decay=1e-3)\n",
        "    optimizer_R = torch.optim.Adam(recognizer.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2), weight_decay=1e-3)\n",
        "    # data.cuda()就转换为GPU的张量类型，torch.cuda.FloatTensor类型\n",
        "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "    TensorB = torch.cuda.ByteTensor if cuda else torch.ByteTensor\n",
        "\n",
        "    # ----------\n",
        "    #  Training\n",
        "    # ----------\n",
        "    epoch = 0\n",
        "    i = 0\n",
        "    eps = 1e-2\n",
        "    gamma = 2.0  #gamma 参数可以看作是被模型选作支持向量的辐射范围的倒数\n",
        "    beta = 10  # 10 for mnist\n",
        "    epoch_num = 0\n",
        "    #Precision从预测结果角度出发，描述了二分类器预测出来的正例结果中有多少是真实正例，\n",
        "    # 即该二分类器预测的正例有多少是准确的\n",
        "    # Precision（精准度），TP/(TP+FP)\n",
        "    all_precision = []\n",
        "    # Recall从真实结果角度出发，描述了测试集中的真实正例有多少被二分类器挑选了出来，\n",
        "    # 即真实的正例有多少被该二分类器召回。\n",
        "    #Recall=TP/(TP+FN)\n",
        "    all_recall = []\n",
        "    # F1得分是统计学中用来衡量二分类模型精确度的一种指标\n",
        "    all_f1 = []\n",
        "    all_acc = []\n",
        "    big_acc = 100\n",
        "    big_f = 0\n",
        "    decrease_epoch = 0\n",
        "    #猜测:创建结果目录，并以csv的格式储存结果\n",
        "    if optimize_type == \"holder\":\n",
        "        if ratio != 1:\n",
        "            filepath = f\"dataframe/{optimize_type}/{opt.dataset}_{loss_type}_{ratio}_{a}.csv\"\n",
        "        else:\n",
        "            filepath = f\"dataframe/{optimize_type}/{opt.dataset}_original_{a}.csv\"\n",
        "    if optimize_type == \"early_stopping\":\n",
        "        filepath = f\"dataframe/{optimize_type}/{opt.dataset}_{optimizer}.csv\"\n",
        "    if optimize_type == \"batch\":\n",
        "        filepath = f\"dataframe/{optimize_type}/{opt.dataset}_{upu_cfg.get('batch_size')}.csv\"\n",
        "    #以新建的方式（会对原有文件进行覆盖），创建文件\n",
        "    f = open(filepath, \"w\")\n",
        "    f.writelines(\"loss_d,loss_r,acc,precision,recall,f1\\n\")\n",
        "\n",
        "    for imgs in training_iterator:\n",
        "        #使用  torch.autograd.set_detect_anomaly(True)​​ 来帮助我们定位具体的出错位置\n",
        "        with torch.autograd.set_detect_anomaly(True):\n",
        "\n",
        "            # Adversarial ground truths\n",
        "            #variable是一种可以不断变化的变量，符合反向传播，参数更新的属性。pytorch的variable是一个存放会变化值的地理位置，里面的值会不停变化，，\n",
        "            # 、pytorch都是由tensor计算的，而tensor里面的参数是variable形式。\n",
        "            imgs_data = Variable(Tensor(imgs[0]), requires_grad=False)\n",
        "            # print(torch.max(imgs_data))\n",
        "            imgs_label = Variable(Tensor(imgs[1]), requires_grad=False)\n",
        "            positive_num = int(torch.sum(imgs_label).data)\n",
        "\n",
        "            # imgs_u_data = Tensor(imgs[0][0])\n",
        "            # imgs_u_label = Tensor(imgs[1][0])/\n",
        "            # imgs_l_data = Tensor(imgs[0][1])\n",
        "            # imgs_l_label = Tensor(imgs[1][1])\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "\n",
        "            if opt.model == 'nnpu':\n",
        "                score_D = discriminator.forward(imgs_data)\n",
        "                score_R = recognizer.forward(imgs_data)\n",
        "                #损失函数\n",
        "                loss_p = 0.5 * torch.sum((1 - score_R) * imgs_label) / torch.sum(imgs_label)\n",
        "                loss_u = torch.sum(score_R * (1 - imgs_label)) / torch.sum(1 - imgs_label) - 0.5 * torch.sum(\n",
        "                    (score_R) * imgs_label) / torch.sum(imgs_label)\n",
        "\n",
        "                objective = loss_p + loss_u\n",
        "                if loss_u.data < -0:\n",
        "                    objective = loss_p - 0\n",
        "                    out = - loss_u\n",
        "                else:\n",
        "                    out = objective\n",
        "                loss_max_d = objective\n",
        "                loss_r = loss_u.clone()\n",
        "                optimizer_R.zero_grad()\n",
        "                out.backward()\n",
        "                optimizer_R.step()\n",
        "\n",
        "            if opt.model == 'upu':\n",
        "                score_D = discriminator.forward(imgs_data)\n",
        "                score_R = recognizer.forward(imgs_data)\n",
        "\n",
        "                loss_p = 0.5 * torch.sum((1 - score_R) * imgs_label) / torch.sum(imgs_label)\n",
        "                loss_u = torch.sum(score_R * (1 - imgs_label)) / torch.sum(1 - imgs_label) - 0.5 * torch.sum(\n",
        "                    (score_R) * imgs_label) / torch.sum(imgs_label)\n",
        "\n",
        "                objective = loss_p + loss_u\n",
        "                # if loss_u.data < -0:\n",
        "                #     objective = loss_p - 0\n",
        "                #     out = - loss_u\n",
        "                # else:\n",
        "                out = objective\n",
        "                loss_max_d = objective\n",
        "                loss_r = loss_u.clone()\n",
        "                #意思是把梯度置零，也就是把loss关于weight的导数变成0\n",
        "                optimizer_R.zero_grad()\n",
        "                #backward函数属于torch.autograd函数库，在深度学习过程中对函数进行反向传播，计算输出变量关于输入变量的梯度。\n",
        "                out.backward()\n",
        "                #执行单个优化步骤(参数更新)\n",
        "                optimizer_R.step()\n",
        "\n",
        "            if opt.model == 'pan':\n",
        "                unlabeled_data = imgs_data[imgs_label.view(-1) == 0]\n",
        "\n",
        "                score_R = recognizer.forward(unlabeled_data)\n",
        "                score_D = discriminator.forward(imgs_data)\n",
        "                # score_R = recognizer.forward(imgs_data)\n",
        "                # 得到区分器discrimation 关于判断为正的得分（猜测）\n",
        "                score_D_p = score_D[imgs_label.view(-1) == 1]\n",
        "                score_D_u = score_D[imgs_label.view(-1) == 0]\n",
        "                #这是一个对齐的作用，因为未标签数据的数量远大于标签数据的数量\n",
        "                score_D_u_h = score_D_u[:score_D_p.shape[0]]\n",
        "                score_R_h = score_R[:score_D_p.shape[0]]\n",
        "\n",
        "                # top-k loss2 (sum)\n",
        "                # kl = (1 * torch.log(1 - score_R + eps + 0.0) - torch.log(score_R + eps + 0.0)) * (2 * score_D_u - 1.0)\n",
        "                # k = int(score_R.shape[0] * ratio)\n",
        "                # kl, indices = torch.topk(kl, k, dim=0)\n",
        "\n",
        "                # top-k KL(Du||Cu)\n",
        "                # kl = score_D_u * torch.log(score_D_u / score_R) + (1 - score_D_u) * torch.log((1 - score_D_u) / (1 - score_R))\n",
        "\n",
        "                # top-k KL(Du||1-Cu)\n",
        "                #  KL散度计算公式（标记1）\n",
        "                #先乘除后加减，有括号先计算括号里面的\n",
        "                kl = score_D_u * torch.log(score_D_u / (1 - score_R)) + (1 - score_D_u) * torch.log((1 - score_D_u) / score_R)\n",
        "                k = int(score_R.shape[0]*ratio)\n",
        "                # 该函数的作用即按字面意思理解，topk:取数组的前k个元素进行排序。dim=0，代表从每一列取出K个元素\n",
        "                # 通常该函数返回2个值，第一个值为排序的数组，第二个值为该数组中获取到的元素在原数组中的位置标号。\n",
        "                kl, indices = torch.topk(kl, k, dim=0, sorted=False, largest=False)\n",
        "                # kl, indices = torch.topk(kl, k, dim=0, sorted=False)\n",
        "                # 根据index索引在input输入张量中选择某些特定的元素，在纵向的维度上\n",
        "                score_R = torch.index_select(score_R, dim=0, index=indices.squeeze())\n",
        "                score_D_u = torch.index_select(score_D_u, dim=0, index=indices.squeeze())\n",
        "                # clone（）返回一个和源张量同shape、dtype和device的张量，与源张量不共享数据内存，但提供梯度的回溯\n",
        "                #detach 意为分离，对某个张量调用函数 detach ()，detach() 的作用是返回一个 Tensor ，它和原张量的数据相同，但不具有梯度属性\n",
        "                # 也就意味detach的方法，将variable参数从网络中隔离开，不参与参数更新\n",
        "                R_rate = score_R.clone().detach()\n",
        "                d_rate = score_D.clone().detach()\n",
        "                d_rate_u = score_D_u_h.clone().detach()\n",
        "                # 猜测\n",
        "                w = 1.0 - torch.exp(- beta * (score_D - 1.0 / gamma) ** 4)\n",
        "                #将一个tensor从创建它的图中分离，并把它设置成叶子tensor\n",
        "                w.detach_()\n",
        "\n",
        "                ######################################################\n",
        "                ######################################################\n",
        "                # please tunning the following hyper-params as different dataset may have different suitable hyper-params.\n",
        "                ######################################################\n",
        "                ######################################################\n",
        "\n",
        "                # pdb.set_trace()\n",
        "                # loss = imgs_label * torch.log(score_D + eps) + 0.01 * (1 - imgs_label) * (1 - R_rate) * torch.log( 1.0 - score_D + eps) + \\\n",
        "                #        0.0003 * w * (1 - imgs_label) * (1 - score_R + eps + 0.0) * torch.tan(1.5 * (gamma * d_rate - 1))\n",
        "                # loss = imgs_label * torch.log(score_D + eps) + 0.01 * (1 - imgs_label) * (1 - R_rate) * torch.log( 1.0 - score_D + eps) + \\\n",
        "                #        0.0003 * w * (1 - imgs_label) * (1 - score_R + eps + 0.0) * torch.tan(1.5 * (gamma * d_rate - 1))\n",
        "                # loss = imgs_label * torch.log(score_D + eps) + 0.000 * (1 - imgs_label) * torch.log(\n",
        "                #    1.0 - score_D + eps) + w * 0.001 * (1 - imgs_label) * (1 * torch.log(1 - score_R + eps + 0.0) - torch.log(score_R + eps + 0.0)) * (score_D - torch.mean(d_rate))\n",
        "                #猜测\n",
        "                loss1 = torch.sum(torch.log(score_D_p + eps)) + 1.0 * torch.sum(\n",
        "                    torch.max(torch.log(1.0 - score_D_u_h + eps) - torch.log(1.0 - torch.mean(d_rate_u)),\n",
        "                              torch.zeros(d_rate_u.shape).cuda()))\n",
        "\n",
        "                loss2 = 0.0001 * torch.sum((1 * torch.log(1 - score_R + eps + 0.0) - torch.log(score_R + eps + 0.0)) * (\n",
        "                        2 * score_D_u - 1.0))  # torch.mean(d_rate_u)\n",
        "\n",
        "                # loss2 = 0.0001 * torch.sum(kl)\n",
        "\n",
        "                loss = loss1 + loss2\n",
        "\n",
        "                # loss = imgs_label * torch.log(score_D + eps) + 0.02 * (1 - imgs_label) * torch.max(torch.log(1.0 - score_D + eps)-torch.log(1.0 - torch.mean(d_rate)), torch.zeros(d_rate.shape).cuda() - 10000.0) + w     * 0.001 * (1 - imgs_label) * (score_D * torch.log(score_D + eps) + (1 - score_D)* torch.log(1 - score_D + eps) - score_D * torch.log(score_R + eps) - (1 - score_D) * torch.log(1 - score_R + eps) )\n",
        "                # loss = imgs_label * torch.log(score_D + eps) + 0.001 * (1 - imgs_label) * torch.log(\n",
        "                #     1.0 - score_D + eps) + w * 0.0001 * (1 - imgs_label) * (1 * torch.log(1 - score_R + eps + 0.0) - torch.log(score_R + eps + 0.0)) * (score_D - torch.mean(d_rate))\n",
        "\n",
        "                # for cifar\n",
        "                # loss = imgs_label * torch.log(score_D + eps) + 0.05 * (1 - imgs_label) * torch.max(torch.log(1.0 - score_D + eps)-torch.log(1.0 - torch.mean(d_rate)), torch.zeros(d_rate.shape).cuda()) + w * 0.0001 * (1 - imgs_label) * (1 * torch.log(1 - score_R + eps + 0.0) - torch.log(score_R + eps + 0.0)) * (2 * score_D - 1.0)\n",
        "\n",
        "                # for sdd\n",
        "                # loss = imgs_label * torch.log(score_D + eps) + 0.1 * (1 - imgs_label) * torch.log(\n",
        "                #     1.0 - score_D + eps) + w * 0.005 * (1 - imgs_label) * (1 * torch.log(1 - score_R + eps + 0.0) - torch.log(score_R + eps + 0.0)) * (score_D - torch.mean(d_rate))\n",
        "\n",
        "                # loss = imgs_label * torch.log(score_D + eps) + 0.01 * (1 - imgs_label) * torch.log(\n",
        "                #     1.0 - score_D + eps) + w * 0.0001 * (1 - imgs_label) * (\n",
        "                #                    torch.log(1 - score_R + eps + 0.0) - 1.0 * torch.log(score_R + eps + 0.0)) * (\n",
        "                #         score_D - torch.mean(d_rate))\n",
        "                #\n",
        "                # loss_1 =  imgs_label * (1.0 - 0.01 * score_R) * torch.log(score_D + eps) \\\n",
        "\n",
        "                # loss_2 = w * (1 - imgs_label) * (score_R + 10 * score_D) * torch.log(gamma - gamma * score_D + eps)\n",
        "                # loss_2 =   w * (1 - imgs_label) * (score_R + 1.0 + eps + 0 * torch.log(score_D + eps)) * torch.log(gamma * score_D + eps)\n",
        "\n",
        "                loss_max_d = - loss\n",
        "                optimizer_D.zero_grad()\n",
        "                loss_max_d.backward(retain_graph=True)\n",
        "                optimizer_D.step()\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Recognizer\n",
        "                # ---------------------\n",
        "                score_R = recognizer.forward(unlabeled_data)\n",
        "                score_D = discriminator.forward(imgs_data)\n",
        "                # score_R = recognizer.forward(imgs_data)\n",
        "                score_D_p = score_D[imgs_label.view(-1) == 1]\n",
        "                score_D_u = score_D[imgs_label.view(-1) == 0]\n",
        "                score_D_u_h = score_D_u[:score_D_p.shape[0]]\n",
        "                score_R_h = score_R[:score_D_p.shape[0]]\n",
        "\n",
        "                # top-k loss2\n",
        "                # kl = (1 * torch.log(1 - score_R + eps + 0.0) - torch.log(score_R + eps + 0.0)) * (2 * score_D_u - 1.0)\n",
        "                # k = int(score_R.shape[0] * ratio)\n",
        "                # kl, indices = torch.topk(kl, k, dim=0)\n",
        "\n",
        "                # top-k KL(Du||Cu)\n",
        "                # kl = score_D_u * torch.log(score_D_u / score_R) + (1 - score_D_u) * torch.log((1 - score_D_u) / (1 - score_R))\n",
        "\n",
        "                # top-k KL(Du||1-Cu)\n",
        "                #KL散度（标记2）\n",
        "                kl = score_D_u * torch.log(score_D_u / (1 - score_R)) + (1 - score_D_u) * torch.log((1 - score_D_u) / score_R)\n",
        "                k = int(score_R.shape[0]*ratio)\n",
        "                kl, indices = torch.topk(kl, k, dim=0, sorted=False, largest=False)\n",
        "                # kl, indices = torch.topk(kl, k, dim=0, sorted=False)\n",
        "                score_R = torch.index_select(score_R, dim=0, index=indices.squeeze())\n",
        "                score_D_u = torch.index_select(score_D_u, dim=0, index=indices.squeeze())\n",
        "\n",
        "                R_rate = score_R.clone().detach()\n",
        "                d_rate = score_D.clone().detach()\n",
        "                d_rate_u = score_D_u_h.clone().detach()\n",
        "                #猜测\n",
        "                loss1 = torch.sum(torch.log(score_D_p + eps)) + 1.0 * torch.sum(\n",
        "                    torch.max(torch.log(1.0 - score_D_u_h + eps) - torch.log(1.0 - torch.mean(d_rate_u)),\n",
        "                              torch.zeros(d_rate_u.shape).cuda()))\n",
        "\n",
        "                loss2 = 0.0001 * torch.sum((1 * torch.log(1 - score_R + eps + 0.0) - torch.log(score_R + eps + 0.0)) * (\n",
        "                            2 * score_D_u - 1.0))  # torch.mean(d_rate_u)\n",
        "\n",
        "                # loss2 = 0.0001 * torch.sum(kl)\n",
        "\n",
        "                loss = loss1 + loss2\n",
        "                loss_r = loss.clone()\n",
        "                optimizer_R.zero_grad()\n",
        "                loss_r.backward()\n",
        "                optimizer_R.step()\n",
        "\n",
        "            if opt.model == 'agan':\n",
        "                score_D = discriminator.forward(imgs_data)\n",
        "                # 对score_D 内的所有元素，取平均值\n",
        "                baseline = torch.mean(score_D).data\n",
        "                # score_R.detach_()\n",
        "                w = 1.0 - torch.exp(- beta * (score_D - 1.0 / gamma) ** 4)\n",
        "                w.detach_()\n",
        "\n",
        "\n",
        "                def cal_co_rl(epoch_):\n",
        "                    if epoch_ > 10:\n",
        "                        return 0.1\n",
        "                    else:\n",
        "                        return 0.0\n",
        "\n",
        "\n",
        "                rl_loss_rate = cal_co_rl(epoch_num)\n",
        "\n",
        "                rl_prob, action_index = rl(score_R)\n",
        "                normal_reward = baseline - score_D\n",
        "                reverse_reward = score_D - baseline\n",
        "\n",
        "                reward = torch.cat((normal_reward, reverse_reward), 1)\n",
        "                reward = torch.sum(reward * action_index, 1).unsqueeze(1)\n",
        "\n",
        "                # pdb.set_trace()\n",
        "                # rl_loss = torch.sum( rl_prob *  torch.log(gamma - gamma * score_D + eps) * rl_loss_rate )\n",
        "                #猜测\n",
        "                rl_loss = torch.sum(rl_prob * reward) * rl_loss_rate\n",
        "                ptd_loss = torch.sum(imgs_label * torch.log(score_D + eps)) + torch.sum(\n",
        "                    (1 - imgs_label[:positive_num]) * torch.log(1 - score_D[:positive_num] + eps))\n",
        "                # ptd_loss = torch.mean( (1-imgs_label) * torch.log(1-score_D + eps) + imgs_label * torch.log(score_D + eps))\n",
        "                # print(ptd_loss.size(),rl_loss.size(),type(rl_loss))\n",
        "                loss = ptd_loss + rl_loss\n",
        "                loss_max_d = - loss\n",
        "                if epoch_num < 2000:\n",
        "                    optimizer_D.zero_grad()\n",
        "                    #retain_graph=True计算梯度所必要的buffer在经历过一次backward过程后不会被释放。\n",
        "                    # 如果你想多次计算某个子图的梯度的时候，设置为True。\n",
        "                    loss_max_d.backward(retain_graph=True)\n",
        "                    optimizer_D.step()\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Recognizer\n",
        "                # ---------------------\n",
        "\n",
        "                optimizer_R.zero_grad()\n",
        "\n",
        "                # loss *= 0.001\n",
        "\n",
        "                loss.backward()\n",
        "                # print(torch.sum(list(recognizer.parameters())[0].grad))\n",
        "                optimizer_R.step()\n",
        "\n",
        "            if training_iterator._finished_epoch != epoch:\n",
        "                epoch_num += 1\n",
        "                # adjust_lr(optimizer_D, epoch_num)\n",
        "                # adjust_lr(optimizer_R, epoch_num)\n",
        "                epoch = training_iterator._finished_epoch\n",
        "                i = 0\n",
        "                testiterator = pu_dataset.get_testing_iterator()\n",
        "                correct = 0.0\n",
        "                all = 0.0\n",
        "                #返回传入字符串的表达式的结果。就是说：将字符串当成有效的表达式 来求值 并 返回计算结果。\n",
        "                recognizer.eval()\n",
        "                # discriminator.eval()\n",
        "                TP = 0.0\n",
        "                FP = 0.0\n",
        "                FN = 0.0\n",
        "                for test_imgs in testiterator:\n",
        "                    test_imgs_data = Variable(Tensor(test_imgs[0]), requires_grad=False)\n",
        "                    test_imgs_label = Variable(TensorB(test_imgs[1]), requires_grad=False)\n",
        "                    # score = discriminator.forward(test_imgs_data)\n",
        "                    score = recognizer.forward(test_imgs_data)\n",
        "                    # pdb.set_trace()\n",
        "                    #gt(a,b)函数比较a中元素大于（这里是严格大于）b中对应元素，大于则为1，不大于则为0，这里a为Tensor\n",
        "                    #bytes() 函数返回字节对象。它可以将对象转换为字节对象，或创建指定大小的空字节对象\n",
        "                    label_predicted = score.gt(0.5).byte() != test_imgs_label\n",
        "                    TP += torch.sum(torch.Tensor.float((score[test_imgs_label == 1] >= 0.5).data))\n",
        "                    FP += torch.sum(torch.Tensor.float((score[test_imgs_label == 0] >= 0.5).data))\n",
        "                    FN += torch.sum(torch.Tensor.float((score[test_imgs_label == 1] <= 0.5).data))\n",
        "                    # all_neg_score.append(score[test_imgs_label < 0.5].cpu().data.numpy())\n",
        "\n",
        "                    label_predicted = torch.Tensor.float(label_predicted.data)\n",
        "                    correct += torch.sum(label_predicted)\n",
        "                    all += float(label_predicted.size(0))\n",
        "                #设置模型为训练模式，即BatchNorm 层利用每个 batch 来统计，Dropout 层激活\n",
        "                recognizer.train()\n",
        "\n",
        "                precision = TP / (TP + FP + 1)\n",
        "                recall = TP / (TP + FN + 1)\n",
        "                # F1得分是统计学中用来衡量二分类模型精确度的一种指标\n",
        "                f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "                # acc = corrects / eval_num\n",
        "\n",
        "                #####################################################\n",
        "\n",
        "                # a , b = torch.max(score,1)\n",
        "                acc = correct / all\n",
        "                # print(acc)\n",
        "                all_precision.append(precision)\n",
        "                all_recall.append(recall)\n",
        "                all_f1.append(f1)\n",
        "                all_acc.append(acc)\n",
        "\n",
        "                print(\"[Epoch %d/%d] [D loss: %f] [R loss: %f] [Accuracy: %f] [precision: %f] [recall: %f] [F1: %f]\" % (\n",
        "                    training_iterator._finished_epoch, opt.n_epochs,\n",
        "                    loss_max_d, loss_r, 1 - acc, precision, recall, f1))\n",
        "\n",
        "                f.writelines(f\"{loss_max_d},{loss_r},{1 - acc},{precision},{recall},{f1}\\n\")\n",
        "\n",
        "                if acc < big_acc:\n",
        "                    big_acc = acc\n",
        "                    decrease_epoch = 0\n",
        "                if f1 > big_f:\n",
        "                    big_f = f1\n",
        "                    decrease_epoch = 0\n",
        "\n",
        "                if acc - big_acc >= 0.01 and big_f - f1 >= 0.01:\n",
        "                    decrease_epoch += 1\n",
        "\n",
        "                if decrease_epoch >= 15:\n",
        "                    break\n",
        "            # batches_done = training_iterator._finished_epoch * training_iterator._len + i\n",
        "            i = i + 1\n",
        "            # if batches_done % opt.sample_interval == 0:\n",
        "    print(loss_type, ratio)\n",
        "    print(\"acc:\", 1 - big_acc)\n",
        "    print(\"f:\", big_f)\n",
        "    print(\"-\")\n",
        "    f.close()\n",
        "\n",
        "    #     save_image(gen_imgs.data[:25], 'images/%d.png' % batches_done, nrow=5, normalize=True)\n",
        "    # result = {'precision': all_precision,\n",
        "    #           'recall': all_recall,\n",
        "    #           'f1': all_f1,\n",
        "    #           'acc': all_acc}\n",
        "    # torch.save(result, opt.dataset + opt.model + '.pkl')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}